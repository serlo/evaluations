{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "856f8916-a873-4286-8adf-3a0f50e153a1",
   "metadata": {},
   "source": [
    "# Find UUIDs we want in the cache\n",
    "\n",
    "Start your local database for this with anonymous data import!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78077c7-369d-40a9-ace2-4f4fac676a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "engine = create_engine(\"mysql+mysqlconnector://root:secret@localhost:3306/serlo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66db6ad0-59ad-45af-beba-fae806f2047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supported (https://github.com/serlo/database-layer/blob/main/server/src/uuid/model/entity/entity_type.rs#L10-L25) and not trashed entities\n",
    "entities = pd.read_sql(\"\"\"\n",
    "    SELECT entity.id\n",
    "    FROM entity\n",
    "    JOIN uuid ON uuid.id = entity.id\n",
    "    JOIN type ON type.id = entity.type_id\n",
    "    WHERE trashed = 0\n",
    "    AND type.name IN (\n",
    "        \"applet\", \"article\", \"course\", \"course-page\", \"event\", \n",
    "        \"text-exercise\", \"text-exercise-group\", \"grouped-text-exercise\", \"text-solution\", \"video\"\n",
    "    )\n",
    "    \"\"\",\n",
    "    con=engine,\n",
    ")\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5246451b-0b31-438e-9f32-1b1d6f42786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current revisions for supported and not trashed for entities\n",
    "current_revisions = pd.read_sql(\"\"\"\n",
    "    SELECT entity.current_revision_id AS id\n",
    "    FROM entity\n",
    "    JOIN uuid ON uuid.id = entity.id\n",
    "    JOIN type ON type.id = entity.type_id\n",
    "    WHERE trashed = 0\n",
    "    AND current_revision_id IS NOT NULL\n",
    "    AND type.name IN (\n",
    "        \"applet\", \"article\", \"course\", \"course-page\", \"event\", \n",
    "        \"text-exercise\", \"text-exercise-group\", \"grouped-text-exercise\", \"text-solution\", \"video\"\n",
    "    )\n",
    "    \"\"\",\n",
    "    con=engine,\n",
    ")\n",
    "current_revisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b4c7ea-4537-48b7-ab05-0d10b975fbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not trashed taxonomies\n",
    "taxonomies = pd.read_sql(\"\"\"\n",
    "    SELECT term_taxonomy.id\n",
    "    FROM term_taxonomy\n",
    "    JOIN uuid ON uuid.id = term_taxonomy.id\n",
    "    WHERE trashed = 0\n",
    "    \"\"\",\n",
    "    con=engine,\n",
    ")\n",
    "taxonomies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae72f36-dc62-44f8-898a-b574220896c8",
   "metadata": {},
   "source": [
    "# Define functions to make GraphQL queries for the UUIDs\n",
    "\n",
    "Here you have to switch, depending on if you want refill the cache of the production or staging environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d34297c-45f1-49c6-a835-3c6ce62133d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAPHQL_API=\"https://api.serlo.org/graphql\"\n",
    "GRAPHQL_API=\"https://api.serlo-staging.dev/graphql\"\n",
    "\n",
    "import requests\n",
    "\n",
    "def api_call(query, variables={}):\n",
    "    req = requests.post(GRAPHQL_API,\n",
    "        headers = { \"Content-Type\": \"application/json\" },\n",
    "        json = { \"query\": query, \"variables\": variables }\n",
    "    )\n",
    "    \n",
    "    return req.json()\n",
    "\n",
    "api_call(\" query { version }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d841a766-74eb-4db0-a774-990ad1007061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uuid(uuid):\n",
    "    return api_call(\"query($uuid: Int!) { uuid(id: $uuid) { __typename }}\", { \"uuid\": uuid })\n",
    "\n",
    "get_uuid(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bfa48b-8fba-4071-9d53-a02b13613e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_cache(row_with_id):\n",
    "    uuid = int(row_with_id['id'])\n",
    "    result = get_uuid(uuid)\n",
    "    \n",
    "    if \"data\" in result and result[\"data\"] != None and result[\"data\"][\"uuid\"] != None:\n",
    "        print(f\"Uuid updated: {uuid}\")\n",
    "    else:\n",
    "        print(f\"No uuid:      {uuid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537f17b6-41f0-4d09-85d2-a3069f117add",
   "metadata": {},
   "source": [
    "# Query UUIDs to get them back into the cache\n",
    "\n",
    "We process the frames with the UUIDS in a parallelized way so it doesn't take forever. \n",
    "Making GraphQL queries is an I/O-bound task, so you could try using even more workers than you have CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6accf973-6573-42b9-b1a7-bcf7801eb710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "# split the long frames in halves to be processed in parallel\n",
    "# if you want it faster, you can try splitting it in even more parts\n",
    "# and give the max_workers argument of ThreadPoolExecutor a higher value\n",
    "half_entities = len(entities) // 2\n",
    "half_revisions = len(current_revisions) // 2\n",
    "uuid_frames = [entities.iloc[:half_entities], entities.iloc[half_entities:], \n",
    "               current_revisions.iloc[:half_revisions], current_revisions.iloc[half_revisions:], \n",
    "               taxonomies]\n",
    "functions = [update_cache] * len(uuid_frames)\n",
    "\n",
    "total_iterations = sum(len(uuid_frame) for uuid_frame in uuid_frames)\n",
    "progress_bar = tqdm(total=total_iterations)\n",
    "\n",
    "def process_dataframe(data_frame, func):\n",
    "    for index, row in data_frame.iterrows():\n",
    "        func(row)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    for uuid_frame, func in zip(uuid_frames, functions):\n",
    "        executor.submit(process_dataframe, uuid_frame, func)\n",
    "\n",
    "progress_bar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
